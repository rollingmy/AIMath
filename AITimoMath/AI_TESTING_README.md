# AI Model Testing Framework

This document describes the comprehensive AI model testing framework integrated into the TIMO Math Learning Engine project.

## ğŸ¯ Overview

The AI testing framework provides comprehensive validation of all AI models used in the adaptive learning system, including:

- **Elo Rating Model** - For difficulty adjustments
- **Bayesian Knowledge Tracing (BKT)** - For concept mastery tracking
- **Item Response Theory (IRT)** - For ability estimation
- **Adaptive Difficulty Engine** - For lesson difficulty recommendations
- **CoreML Models** - For question recommendations and predictions

## ğŸ“ File Structure

```
AITimoMath/
â”œâ”€â”€ MLTraining/
â”‚   â”œâ”€â”€ ComprehensiveAITests.swift      # Main testing framework
â”‚   â”œâ”€â”€ TestResultsAnalyzer.swift       # Results analysis and reporting
â”‚   â”œâ”€â”€ RunAccuracyTests.swift          # Test execution scripts
â”‚   â””â”€â”€ test_ai_models.swift            # Updated with new integration
â”œâ”€â”€ TestRunners/
â”‚   â””â”€â”€ AIModelTestRunner.swift         # Updated test runner
â”œâ”€â”€ Views/
â”‚   â””â”€â”€ AITestingView.swift             # UI for running tests
â””â”€â”€ run_ai_tests.sh                     # Command line script
```

## ğŸš€ Quick Start

### 1. Command Line Usage

```bash
# Navigate to project root
cd /path/to/AITimoMath

# Run all tests
./run_ai_tests.sh all

# Run only accuracy tests
./run_ai_tests.sh accuracy

# Run basic functionality tests
./run_ai_tests.sh basic

# Run with custom configuration
./run_ai_tests.sh custom 0.9 1000
```

### 2. Programmatic Usage

```swift
// Run comprehensive accuracy tests
let comprehensiveTests = ComprehensiveAITests()
let results = comprehensiveTests.runAllAccuracyTests()

// Generate analysis report
let analysisReport = TestResultsAnalyzer.generateReport(results)
print(analysisReport)

// Run via test runner
AIModelTestRunner.runAccuracyTests()
```

### 3. UI Usage

Add the `AITestingView` to your app's debug menu or settings:

```swift
// In your debug/settings view
NavigationLink("AI Model Tests") {
    AITestingView()
}
```

## ğŸ§ª Test Types

### 1. Basic Functionality Tests
- Validates that all AI models can be instantiated
- Tests basic method calls and parameter validation
- Ensures models return expected data types

### 2. Accuracy Tests
- **Elo Rating Model**: Tests rating calculations, difficulty conversions
- **BKT Model**: Tests knowledge updates, mastery determination
- **IRT Model**: Tests probability calculations, ability estimation
- **Adaptive Difficulty Engine**: Tests difficulty recommendations
- **CoreML Models**: Tests predictions and recommendations

### 3. Comprehensive Test Suite
- Combines basic and accuracy tests
- Generates detailed analysis reports
- Provides deployment readiness assessment

## ğŸ“Š Test Results

### Accuracy Thresholds
- **Excellent**: 95%+ accuracy
- **Good**: 85-94% accuracy
- **Acceptable**: 75-84% accuracy
- **Poor**: 60-74% accuracy
- **Critical**: <60% accuracy

### Deployment Readiness
- **Ready**: All models pass, high confidence
- **Conditional**: Most models pass, some concerns
- **Not Ready**: Significant issues found
- **Critical**: Major failures detected

## ğŸ“ˆ Expected Results

Based on the AI models in your system, you should expect:

| Model | Expected Accuracy | Notes |
|-------|------------------|-------|
| Elo Rating Model | 90-95% | Well-established algorithm |
| BKT Model | 85-90% | Depends on parameter tuning |
| IRT Model | 88-92% | Mathematical model, very reliable |
| Adaptive Difficulty Engine | 80-85% | Complex integration of multiple models |
| CoreML Models | 75-85% | Depends on training data quality |

## ğŸ”§ Configuration

### Custom Test Configuration

```swift
// Run tests with custom parameters
RunAccuracyTests.executeWithConfiguration(
    accuracyThreshold: 0.9,    // 90% threshold
    testIterations: 500,       // 500 test iterations
    saveResults: true          // Save results to file
)
```

### Test Parameters

- **accuracyThreshold**: Minimum accuracy required (0.5-0.95)
- **testIterations**: Number of test iterations (100-2000)
- **tolerance**: Tolerance for predictions (default: 0.05)

## ğŸ“‹ Test Reports

### Console Output
The framework provides detailed console output including:
- Test progress indicators
- Individual model results
- Overall summary statistics
- Deployment readiness assessment

### File Output
Test results are automatically saved to:
- `Documents/AI_Model_Test_Results.json` - Basic results
- `Documents/AI_Accuracy_Test_Results_[timestamp].json` - Detailed results

### Analysis Reports
Comprehensive analysis reports include:
- Model performance breakdown
- Issue identification
- Improvement recommendations
- Trend analysis (when historical data available)

## ğŸ› ï¸ Integration

### With Existing Test Infrastructure

The framework integrates seamlessly with your existing test infrastructure:

```swift
// In your existing test files
import ComprehensiveAITests

// Run as part of existing test suite
let allPassed = RunAccuracyTests.runAsPartOfTestSuite()

// Get results for CI/CD integration
let ciResults = RunAccuracyTests.getTestResultsForCI()
```

### With CI/CD Pipeline

```swift
// For automated testing
let results = RunAccuracyTests.getTestResultsForCI()
let deploymentReady = results["summary"]?["deploymentReady"] as? Bool ?? false

if !deploymentReady {
    // Fail the build or send notification
    exit(1)
}
```

## ğŸ› Troubleshooting

### Common Issues

1. **Build Errors**
   - Ensure all AI model files are included in the target
   - Check that CoreML models are properly added to the bundle

2. **Test Failures**
   - Review the detailed test output for specific failure reasons
   - Check model parameters and configuration
   - Verify test data quality

3. **Performance Issues**
   - Reduce test iterations for faster execution
   - Run tests on background threads
   - Consider running only critical tests in CI/CD

### Debug Mode

Enable debug logging by setting:

```swift
// In your app configuration
UserDefaults.standard.set(true, forKey: "AI_Testing_Debug_Mode")
```

## ğŸ“š API Reference

### ComprehensiveAITests

```swift
class ComprehensiveAITests {
    func runAllAccuracyTests() -> [String: TestResult]
}
```

### TestResultsAnalyzer

```swift
class TestResultsAnalyzer {
    static func analyzeResults(_ results: [String: TestResult]) -> AnalysisResult
    static func generateReport(_ results: [String: TestResult]) -> String
}
```

### AIModelTestRunner

```swift
class AIModelTestRunner {
    func runTests() -> [String]
    func runAccuracyTests() -> [String]
    func runAllTests() -> [String]
}
```

## ğŸ”„ Continuous Improvement

### Adding New Tests

1. Add test cases to `ComprehensiveAITests.swift`
2. Update expected results in documentation
3. Add new models to the test suite
4. Update analysis logic in `TestResultsAnalyzer.swift`

### Monitoring Trends

The framework is designed to support trend analysis:
- Historical result comparison
- Performance regression detection
- Model improvement tracking

## ğŸ“ Support

For issues or questions about the AI testing framework:

1. Check the console output for detailed error messages
2. Review the generated analysis reports
3. Examine the saved test result files
4. Consult the API reference above

## ğŸ‰ Success Criteria

Your AI models are ready for deployment when:

- âœ… All models achieve â‰¥85% accuracy
- âœ… Overall success rate â‰¥80%
- âœ… No critical failures detected
- âœ… All recommendations addressed
- âœ… Performance meets requirements

---

*This testing framework ensures your AI models meet the high standards required for educational applications, providing confidence in the adaptive learning system's effectiveness.*
